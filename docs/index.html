<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title property="foaf:name schema:name">Mapping Change: A Temporal and Semantic Knowledge Base of Scottish Gazetteers (1803–1901)</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print" href="styles/strict-print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  <meta name="citation_title" content="Mapping Change: A Temporal and Semantic Knowledge Base of Scottish Gazetteers (1803–1901)">
  
  
  <meta name="citation_publication_date" content="2025/05/08" />
</head>

<body prefix="dctypes: http://purl.org/dc/dcmitype/ pimspace: http://www.w3.org/ns/pim/space# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# lsc: http://linkedscience.org/lsc/ns#" typeof="schema:CreativeWork sioc:Post prov:Entity lsc:Research">
  <header>
  <h1>Mapping Change: A Temporal and Semantic Knowledge Base of Scottish Gazetteers (1803–1901)</h1>
  <p>
    <strong>Lilin Yu</strong>, <strong>Rosa Filgueira</strong><br />
    EPCC, University of Edinburgh<br />
    <a href="mailto:L.Yu-40@sms.ed.ac.uk">L.Yu-40@sms.ed.ac.uk</a>,
    <a href="mailto:r.filgueira@epcc.ed.ac.uk">r.filgueira@epcc.ed.ac.uk</a>
  </p>
  <p><strong>Identifier:</strong> <a href="https://rosafilgueira.github.io/MappingChange-Paper-ISWC2025/">https:/​/​rosafilgueira.github.io/MappingChange-Paper-ISWC2025/</a></p>
</header>

<!-- Hack to make our custom fonts load in print-mode -->
<p><span class="printfont1"> </span>
<span class="printfont2"> </span>
<span class="printfont3"> </span>
<span class="printfont4"> </span></p>

<div id="content">
  <section id="abstract" inlist="" rel="schema:hasPart" resource="#abstract">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Abstract</h2>

      <p>We present <strong><a href="https://github.com/francesNLP/MappingChange">MappingChange</a></strong>, a resource that constructs a temporal and semantic knowledge base from ten 19th-century <strong><a href="https://data.nls.uk/data/digitised-collections/gazetteers-of-scotland/">Gazetteers of Scotland</a></strong> (1803–1901), digitized as over 13,000 page-level <strong>ALTO XML</strong> files. These texts are noisy, inconsistently structured, and lack article-level markup—posing substantial challenges for reuse. To segment and extract over 50,000 historical place descriptions, we employ <strong>large language models (LLMs)</strong> with <strong>edition-specific prompt strategies</strong>, tuned to handle diverse editorial conventions, abbreviations, recurring place names, and multi-page entries. The resulting knowledge base comprises three interlinked knowledge graphs: (1) a <strong>basic KG</strong>, representing extracted article-level entries and their metadata; (2) a <strong>concept-enriched KG</strong>, clustering semantically related descriptions across editions using <strong>sentence embeddings</strong>, with links to <strong>Wikidata</strong> and <strong>DBpedia</strong>; and (3) a <strong>location-annotated KG</strong>, enriched through <strong>named entity recognition</strong> and <strong>georesolution</strong>. All graphs are expressed in <strong><a href="https://www.w3.org/RDF/">RDF</a></strong> and modeled using the extended <strong><a href="https://w3id.org/hto">Heritage Textual Ontology (HTO)</a></strong>, which captures textual provenance, extraction context, and diachronic alignment. In addition to the knowledge graphs, we release (a) edition-specific and aggregated DataFrames (one row per place entry, including metadata and embeddings), and (b) a set of Jupyter Notebooks illustrating usage of our resource. All components are openly available and deployed through the <a href="http://www.frances-ai.com">Frances</a> semantic platform and a public <a href="http://query.frances-ai.com/hto_gazetteers">Fuseki endpoint</a>, enabling historical exploration via semantic search, timeline comparison, and visual querying interfaces.</p>

    </div>
</section>


<main>
  <section id="introduction" inlist="" rel="schema:hasPart" resource="#introduction">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Introduction</h2>

        <p>Descriptive gazetteers were a cornerstone of how 19th-century Scotland recorded and transmitted its geographical knowledge—capturing towns, parishes, rivers, castles, and natural features within evolving historical, social, and economic narratives. As industrialization, migration, land reform, and empire reshaped the country, these texts became critical instruments for documenting change. The <em>Gazetteers of Scotland, 1803–1901</em>, digitized by the <a href="https://data.nls.uk/data/digitised-collections">National Library of Scotland (NLS)</a>, form one of the most extensive corpora for studying this spatial transformation. Yet despite being released as more than 13,000 OCR-aligned ALTO XML files, the collection remains largely unsuitable for structured analysis. The lack of article-level markup, noisy layouts, and inconsistent editorial conventions limit its accessibility for digital scholarship.</p>

        <p>Each edition introduces distinct typographic conventions for article headers, redirects, abbreviations, and multi-page entries. In early volumes, place names appear in uppercase with minimal punctuation to separate entries (Figure 1, left), whereas later editions use clearer formatting, including title casing and consistent delimiters (Figure 1, right). These variations complicate traditional rule-based approaches to text segmentation and alignment, making it difficult to trace how a place is described across time or to resolve recurring names that refer to different locations.</p>

        <p>Compounding these challenges is the fact that many place names (e.g. “ABBEY”) recur across the gazetteers, often referring to different locations. Moreover, later editions tend to include a broader set of places, meaning some names appear for the first time in later volumes or gain more detailed descriptions over time. Disambiguating such entries is non-trivial, as it depends on contextual clues within each article rather than surface-level patterns. Our approach relies on LLM-based article segmentation and interpretation—capturing subtle editorial cues and semantic context to associate each name with the appropriate description.</p>

        <p>To overcome these limitations, we present <strong>MappingChange</strong>, a reusable and openly licensed resource that transforms this historically rich but structurally fragmented corpus into a structured and semantically enriched knowledge base. Our pipeline uses large language models (LLMs) with edition-specific prompt strategies to extract over 50,000 article-level entries and express them in RDF as three interlinked knowledge graphs, modeled using the <a href="https://w3id.org/hto">Heritage Textual Ontology (HTO)</a>. The resulting knowledge base supports both <strong>temporal analysis</strong>, enabling comparison of how place descriptions change across editions, and <strong>semantic exploration</strong>, through the linking of conceptually related entries and connections to <a href="https://www.wikidata.org">Wikidata</a> and <a href="https://www.dbpedia.org">DBpedia</a>.</p>

        <p>A full account of the dataset structure, file formats, and access methods is provided in <a href="#resourcedescription">Section 3</a>. All resources (DataFrames, knowledge graphs, and Notebooks) are publicly archived and distributed via our <a href="github.com/francesNLP/MappingChange">repository</a>, with persistent identifiers issued via Zenodo and integrated into the <a href="http://www.frances-ai.com">Frances</a> semantic web platform for visual exploration.</p>

        <div style="display: flex; justify-content: space-between; align-items: flex-start; gap: 1em;">
  <div style="flex: 1; text-align: center;">
    <img src="images/1803-gazetteer-page.jpg" alt="First page of the 1803 Gazetteer of Scotland" style="max-width: 180px; height: auto; border: 1px solid #ccc;" />
    <p style="font-size: 0.9em;">
      <strong>Figure 1 (left):</strong> Opening entries of the 1803 <em>Gazetteer of Scotland</em>. Page headers consist of two three-letter uppercase segments (e.g., “ABB ABE”). Place names appear in all caps, typically followed by a period or semicolon—offering minimal typographic separation between entries.
    </p>
  </div>
  <div style="flex: 1; text-align: center;">
    <img src="images/1884-gazetteer-page.jpg" alt="First page of the 1884 Ordnance Gazetteer of Scotland" style="max-width: 200px; height: auto; border: 1px solid #ccc;" />
    <p style="font-size: 0.9em;">
        <strong>Figure 1 (right):</strong> Opening entries of the 1884 <em>Ordnance Gazetteer of Scotland</em>. This edition features a clearer visual structure, with entries formatted in title case and followed by commas. Page headers display the first and last place names on the page, both rendered in uppercase.
    </p>
  </div>
</div>

        <p>All resources presented in tis work are reproducible from source using openly available scripts, which are detailed in <a href="#resourceconstruction">Section 5</a>. This enables other researchers to reuse, adapt, or extend the MappingChange infrastructure for new corpora, ensuring FAIR data practices and long-term sustainability. In doing so, the project facilitates new forms of historical geography and cultural analytics by making a corpus computationally interoperable, queryable, and reusable across domains.
/
The remainder of this paper is structured as follows. <a href="#relatedwork">Section 2</a> reviews related work on Semantic Web methods for cultural heritage, including digitization, ontology design, and the use of large language model. <a href="#resourcedescription">Section 3</a> provides a detailed description of the MappingChange resource. <a href="#hto">Section 4</a> details the Heritage Textual Ontology. <a href="#resourceconstruction">Section 5</a> outlines the end-to-end pipeline for extraction, cleaning, and semantic enrichment. <a href="#usage">Section 6</a> presents usage scenarios and queries that demonstrate how the resource supports historical research. Finally, <a href="#conclusion">Section 7</a> concludes with a summary of contributions and future directions.</p>

      </div>
</section>

  <section id="relatedwork" inlist="" rel="schema:hasPart" resource="#relatedwork">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Related Work</h2>

        <p>A growing body of research in digital humanities and cultural heritage has increasingly adopted Semantic Web technologies to structure, enrich, and interlink historical textual corpora. Notable examples include <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.3233/SW-223034"><a href="  https://journals.sagepub.com/doi/abs/10.3233/SW-223034 ">WarSampo</a></span> <span class="references">[<a href="#ref-1">1</a>]</span>, which models World War II data from Finland as Linked Open Data, and <a href="https://enslaved.org">Enslaved.org</a> (ontolgy available <a href="https://docs.enslaved.org/ontology">here</a>), which applies graph-based modeling to records from the transatlantic slave trade. These projects demonstrate how knowledge graphs can be used to represent complex relationships among people, places, and events in heterogeneous historical sources.</p>

        <p>Initiatives such as the Europeana Data Model <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1177/03400352231202506"><a href="https://doi.org/10.1177/03400352231202506">EDM</a></span> <span class="references">[<a href="#ref-2">2</a>]</span> and the National Library of Scotland’s <a href="https://data.nls.uk/data/digitised-collections/">Data Foundry</a> exemplify large-scale digitization and metadata modeling efforts aimed at improving accessibility and reuse of cultural heritage data. Europeana promotes interoperability through linked data principles and vocabulary standardization, while the NLS provides high-quality scans and <a href="https://altoxml.github.io">ALTO XML</a> and <a href="https://www.loc.gov/standards/mets/">METS XML</a> for thousands of 19th-century documents, including the <a href="https://data.nls.uk/data/digitised-collections/gazetteers-of-scotland/">Gazetteers of Scotland</a>. However, these infrastructures alone are insufficient for corpora like Scottish gazetteers, which present significant challenges: noisy OCR, lack of article-level segmentation, mid-page article starts, and inconsistent editorial conventions across editions. Traditional approaches to structuring such texts—including rule-based or statistical methods—often fail under these conditions. Prior work on historical textual collections, such as newspapers or the Encyclopaedia Britannica (e.g., using the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1109/eScience51609.2021.00012"><a href="https://doi.org/10.1109/eScience51609.2021.00012">defoe library</a></span> <span class="references">[<a href="#ref-3">3</a>]</span>) has demonstrated the need for scalable, domain-adapted pipelines.</p>

        <p>Recent breakthroughs in large language models (LLMs) such as <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.48550/arXiv.2303.08774"><a href="https://arxiv.org/abs/2303.08774">GPT-4</a></span> <span class="references">[<a href="#ref-4">4</a>]</span> open new possibilities for flexible text interpretation and segmentation. Our project leverages these models at scale with custom prompts tailored to the editorial style of each edition. This enables us to segment and extract over 50,000 structured article-level entries from ten 19th-century gazetteer volumes, while handling abbreviation styles, redirects, and evolving toponym usage.</p>

        <p>Beyond extraction, semantic modeling is critical to ensuring data reusability and interpretability. We build on and extend prior ontologies developed for cultural heritage contexts—such as the Encyclopaedia Britannica Ontology <span class="references">[<a href="#ref-5">5</a>]</span> and the National Library of Scotland Ontology <span class="references">[<a href="#ref-6">6</a>]</span>—to model bibliographic provenance and source structure. However, these earlier ontologies were not designed to represent the full diachronic and computational transformation history of digitized corpora. To address this gap, we introduced the <a href="https://w3id.org/hto">Heritage Textual Ontology</a> (HTO <span class="references">[<a href="#ref-7">7</a>]</span>), which provides a provenance-aware semantic framework that models not only the source structure and bibliographic metadata, but also digitization context, Name Entity Recognition (NER)-based outputs, and semantic enrichments. HTO integrates concepts from <a property="schema:citation http://purl.org/spar/cito/cites" href="https://www.w3.org/TR/prov-o/">PROV-O</a> <span class="references">[<a href="#ref-8">8</a>]</span> and <a href="https://schema.org">Schema.org</a>, but extends them with domain-specific classes and properties tailored to heritage corpora, supporting integration of various data sources from the same corpora.</p>

        <p>Finally, this work is fully integrated into the <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.1109/E-SCIENCE62913.2024.10678663"><a href="https://doi.org/10.1109/e-Science62913.2024.10678663">Frances</a></span> <span class="references">[<a href="#ref-9">9</a>]</span> semantic platform, which supports temporal exploration and semantic search of historical data. Recent updates to <em>Frances</em> include improved support for collection browsing, semantic and full-text search, concept evolution over time, and geolocation visualization, making MappingChange a robust and reusable infrastructure for temporal knowledge base construction in historical research.</p>

      </div>
</section>

  <section id="resourcedescription" inlist="" rel="schema:hasPart" resource="#resourcedescription">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Resource Description</h2>

        <p>The MappingChange resource provides a reusable, modular knowledge base that transforms the digitized <a href="https://data.nls.uk/data/metadata-collections/gazetteers-of-scotland/">Gazetteers of Scotland</a> (1803–1901) into a structured and semantically enriched dataset for historical place-based analysis. Central to the resource are volume-specific JSON DataFrames containing over 50,000 articles—each corresponding to a place entry—extracted from OCR-aligned ALTO XML using edition-specific GPT-4 prompts. These DataFrames include article text, identified place names, and structured metadata such as edition, volume, page number, and word count. Each DataFrame is normalized to a one-entry-per-row format, making it suitable for downstream integration and statistical analysis. We provide both individual DataFrames for each of the ten editions and a unified, cross-edition DataFrame that aggregates all entries into a single structure.</p>

        <p>These DataFrames serve as the foundation for constructing three interlinked RDF knowledge graphs, modeled using the <a href="https://w3id.org/hto">Heritage Textual Ontology (HTO)</a>, which captures both textual provenance and semantic transformations. The first knowledge graph encodes cleaned article-level records and their bibliographic context, preserving references to their original digitized pages. The second graph includes pre-computed sentence embeddings for each article and introduces concept-level alignment by clustering semantically equivalent entries across editions, assigning persistent identifiers and linking them to external entities such as <a href="https://www.wikidata.org">Wikidata</a> and <a href="https://www.dbpedia.org">DBpedia</a>. The third graph adds geographic enrichment, using named entity recognition and georesolution techniques to annotate articles with location coordinates and spatial types. Together, these layers enable advanced temporal and semantic analysis of how places were described over time in the Scottish Gazetteers.</p>

        <p>All knowledge graphs are serialized in RDF/Turtle and adhere to FAIR data principles. Outputs are hosted in a public <a href="http://query.frances-ai.com/hto_gazetteers">Fuseki SPARQL endpoint</a>, and are complemented by a curated set of Jupyter notebooks. These include SPARQL query examples and data exploration analyses. The three interlinked knowledge graphs are deployed within the <a href="http://www.frances-ai.com">Frances</a> semantic platform, providing users with interactive interfaces for querying, timeline visualization, and concept exploration.</p>

        <p>The full resource (including individual and aggregated DataFrames, knowledge graphs, and exploratory notebooks) is openly available at <a href="https://github.com/francesNLP/MappingChange">MappingChange repository</a>, with persistent identifiers to be issued via Zenodo. To promote reuse and reproducibility, the repository includes modular scripts and comprehensive documentation. The step-by-step process by which the data and knowledge graphs are constructed—including article extraction, enrichment, and RDF serialization—is described in <a href="#resourceconstruction">Section 5</a>.</p>

      </div>
</section>

  <section id="hto" inlist="" rel="schema:hasPart" resource="#hto">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Heritage Textual Ontology (HTO)</h2>

        <p>The <a href="https://w3id.org/hto">HTO</a> provides the semantic backbone for <em>MappingChange</em>, enabling the structured representation of historical textual records, their provenance, and the evolving concepts they describe. Developed to support real-world use cases in digital heritage, HTO models not only entities and attributes, but also the editorial and computational processes by which historical texts are extracted, digitized, interpreted, and semantically enriched. Unlike more generic vocabularies, HTO is tailored to the challenges of heritage corpora—such as OCR noise, editorial variation, and evolving terminology—offering fine-grained support for provenance, textual quality, named entity recognition, and diachronic conceptual alignment.</p>

        <p>HTO is <a href="https://github.com/frances-ai/HeritageTextOntology">openly developed</a>, and it builds on established ontologies such as <a href="https://www.w3.org/TR/prov-o/">PROV-O</a>, <a href="https://www.w3.org/TR/skos-reference/">SKOS</a>, <a href="https://schema.org">Schema.org</a>, and <a href="https://www.cidoc-crm.org">CIDOC CRM</a>, while introducing domain-specific classes and properties designed for flexible reuse and extension. Since its initial release, HTO has been extended to support geospatial annotation, richer NER-based enrichment, and explicit modeling of language-model-based transformations, making it suitable for a wider range of digitized corpora.</p>

        <p>HTO is modular and extensible, and can be adopted in other projects that require modeling of OCR-derived documents, provenance-aware digitization pipelines, or diachronic semantic alignment. It also plays a central role in enabling FAIR knowledge graph construction with transparent lineage tracking.</p>

        <h3 id="modeling-bibliographic-structure-and-provenance">Modeling Bibliographic Structure and Provenance</h3>

        <p>HTO provides a structured vocabulary for modeling the archival hierarchy of heritage texts (including works, editions, series, and pages) through the class <code>hto:Work</code> and its subclasses. Each textual entity is also a <code>prov:Entity</code> (<code>hto:EntityWithProvenance</code>), enabling provenance tracking for digitization methods (e.g., OCR or manual transcription), quality levels (<code>hto:TextQuality</code>), and attribution to specific software or human agents (<code>prov:Agent</code>). Works can be grouped into collections using <code>hto:Collection</code> and linked to their physical or digital source editions. Figure <a href="#fig-hto-bib">2</a> illustrates this bibliographic modeling layer.</p>
        <p align="center">
  <img src="images/HTO_textual_content.png" alt="HTO bibliographic modeling" width="400px" />
</p>
        <p align="center" id="fig-hto-bib"><strong>Figure 2:</strong> Bibliographic and provenance modeling in HTO, including core classes (blue), agents (orange), locations (green), and datatype properties (grey).</p>

        <h3 id="capturing-textual-records-and-interpretations">Capturing Textual Records and Interpretations</h3>

        <p>HTO distinguishes between original descriptions (<code>hto:OriginalDescription</code>) and derived or enriched interpretations (<code>hto:Description</code>). These are associated with their source pages via <code>hto:Page</code> and <code>hto:hasOriginalDescription</code>, and annotated with quality levels (e.g., “Low”, “Moderate”, or “High”) based on their provenance. This allows the knowledge base to retain multiple text variants and trace how they were extracted and transformed.</p>

        <p>Records within the text are modeled using <code>hto:InternalRecord</code> (for local entities) and <code>hto:ExternalRecord</code> (for linked data resources such as Wikidata or DBpedia). Repeated terms (like “St Andrews”) are tracked across editions via <code>hto:TermRecord</code>, while <code>hto:ConceptRecord</code> groups semantically similar entries into shared <code>hto:Concept</code>s, enabling diachronic alignment. Figure <a href="#fig-hto-terms">3</a> shows how textual terms and their semantic clusters are represented.</p>
        <p align="center">
  <img src="images/Annotation.png" alt="HTO term modeling" width="400px" />
</p>
        <p align="center" id="fig-hto-terms"><strong>Figure 3:</strong> Modeling of term records and concept clusters using HTO classes for internal and external alignment.</p>

        <h3 id="geospatial-annotations-and-place-modeling">Geospatial Annotations and Place Modeling</h3>

        <p>In <em>MappingChange</em>, HTO has been extended to support spatial annotation and georesolution (See Figure <a href="#fig-hto-geo">4</a>). Locations are represented as <code>hto:Location</code> and typed using subclasses such as <code>hto:Town</code>, <code>hto:Region</code>, or <code>hto:River</code>. Each place may include spatial geometries using <code>geo:hasGeometry</code> or <code>geo:asGeoJSON</code>, with optional declarations using <code>cidoc-crm:SP2_Declarative_Place</code>. This enables spatial reasoning, integration with GeoSPARQL, and alignment with modern gazetteers and linked data services. or an overview of spatial modeling in HTO.</p>

        <p align="center">
  <img src="images/Geospatial.png" alt="HTO geospatial modeling" width="400px" />
</p>
        <p align="center" id="fig-hto-geo"><strong>Figure 4:</strong> Representation of locations and spatial types in HTO, including georesolved coordinates and place categorization.</p>

        <h3 id="annotation-and-segment-selection">Annotation and Segment Selection</h3>

        <p>To link extracted place names or descriptions to their textual anchors, HTO adopts <a href="https://www.w3.org/TR/annotation-model/">Web Annotation (OA)</a> standards. Each annotation (<code>oa:Annotation</code>) includes a source document, a target entity, and selectors (e.g., <code>oa:TextQuoteSelector</code>, <code>oa:TextPositionSelector</code>) that specify the exact span of text involved. This ensures fine-grained traceability back to OCR-aligned sources and supports downstream validation and curation. An example is shown in Figure <a href="#fig-hto-annotation">5</a>.</p>

        <p align="center">
  <img src="images/Annotation_example.png" alt="HTO annotation example" width="400px" />
</p>
        <p align="center" id="fig-hto-annotation"><strong>Figure 5:</strong> Example of how text segments are annotated and anchored using OA selectors in HTO.</p>

      </div>
</section>

  <section id="resourceconstruction" inlist="" rel="schema:hasPart" resource="#resourceconstruction">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Construction and Content</h2>

        <p>The <em>MappingChange</em> knowledge base is built through a multi-stage pipeline that transforms unstructured OCR-aligned ALTO XML files into structured DataFrames and RDF graphs. Each stage of the pipeline is modular and reproducible, with edition-specific scripts documented and openly available in the <a href="https://github.com/francesNLP/MappingChange">MappingChange GitHub repository</a>.</p>

        <p>Of the twelve editions in the <a href="https://data.nls.uk/data/digitised-collections/gazetteers-of-scotland/">NLS collection</a>, we process ten (see <a href="#fig-gz-vols">Figure 6</a>) as fully descriptive gazetteers with complete metadata and multi-volume structure where applicable. We exclude the 1828 edition, which is a town-focused summary rather than a gazetteer, and the 1848 edition, for which only Volume II is available. These ten editions form the basis of the MappingChange resource, which segments article-level entries from each page of OCR-aligned text. This structured segmentation enables precise extraction of place names and their descriptions—supporting downstream tasks such as cross-edition comparison, named entity linking, and interactive historical exploration.</p>

        <p align="center">
  <img src="images/gazetteers_vols.png" alt="Number of Gazetteer Volumes Per Year" style="max-width: 400px; height: auto; border: 1px solid #ccc;" />
</p>
        <p align="center" id="fig-gz-vols"><strong>Figure 6:</strong> Number of volumes per gazetteer edition (1803–1901). The 1883 edition spans six volumes, while most others are single- or double-volume works.</p>

        <h3 id="article-extraction-and-prompt-engineering">Article Extraction and Prompt Engineering</h3>

        <p>As input to our pipeline, we use the <a href="https://drive.google.com/file/d/1J6TxdKImw2rNgmdUBN19h202gl-iYupn/view?usp=share_link">gazetteers_dataframe</a>, a consolidated DataFrame derived from our earlier <span property="schema:citation http://purl.org/spar/cito/cites" resource="https://dx.doi.org/10.5281/zenodo.14051678"><a href="https://doi.org/10.5281/zenodo.14051678">Gazetteer_HTO knowledge graph</a></span> <span class="references">[<a href="#ref-10">10</a>]</span>. This resource contains entries from the ten selected editions, each representing the full OCR text of a page along with metadata such as edition identifier, volume, page number, and candidate place names.</p>

        <p>The goal of this stage is to extract structured article-level place descriptions. This task presents several challenges: (a) place names can be ambiguous or repeated across editions; (b) many descriptions span multiple pages; (c) some places are introduced only in later editions; (d) entries frequently include references to other places (e.g., <em>“See Paisley”</em>); and (e) alternative place names must be captured. Our custom GPT-4 prompts and scripts are designed to address these challenges by isolating each named place and its full contextual description, including cross-references and aliases.</p>

        <p>We use edition-specific Python scripts (<code>extract_gaz_*.py</code>) to segment and extract articles. Each script tailors the GPT-4 prompt to the unique typographic and editorial conventions of the edition—handling mid-page redirects, abbreviation styles, header formats, and layout variations.</p>

        <p>The table below summarizes key layout features and corresponding prompt adjustments:</p>

        <table>
          <thead>
            <tr>
              <th><strong>Edition</strong></th>
              <th><strong>Layout/Format Features</strong></th>
              <th><strong>Prompt Adjustments</strong></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>1803</td>
              <td>All caps titles, minimal punctuation, mid-column entries</td>
              <td>Prompt includes rules for semicolon-delimited entries</td>
            </tr>
            <tr>
              <td>1806</td>
              <td>Similar to 1803 with improved spacing</td>
              <td>Added regex pre-filters to exclude 3-letter headers</td>
            </tr>
            <tr>
              <td>1825</td>
              <td>Shorter entries, denser formatting</td>
              <td>Emphasis on short entries and abbreviation disambiguation</td>
            </tr>
            <tr>
              <td>1838</td>
              <td>Two-column format, clearer title separation</td>
              <td>Prompt refined to distinguish article breaks explicitly</td>
            </tr>
            <tr>
              <td>1842</td>
              <td>Redirects common, layout noisy</td>
              <td>Includes logic for disambiguating abbreviated redirects</td>
            </tr>
            <tr>
              <td>1846</td>
              <td>Continued abbreviation patterns, multi-page entries</td>
              <td>Includes continuity checks and redirect expansion</td>
            </tr>
            <tr>
              <td>1868</td>
              <td>Longer, structured entries with location hierarchies</td>
              <td>Added cues for nested article types and locations</td>
            </tr>
            <tr>
              <td>1884–1901</td>
              <td>Title-cased entries, structured and clean layout</td>
              <td>Simplified prompts; uses typographic features directly</td>
            </tr>
          </tbody>
        </table>

        <p>The scripts tokenize OCR text by page, apply prompts in batch mode, and parse outputs into structured JSON files. These are then aggregated into edition-specific DataFrames—each with one row per article.</p>

        <h3 id="dataframes-to-rdf">DataFrames to RDF</h3>

        <p>Each DataFrame is cleaned and converted to RDF using the <a href="https://w3id.org/hto">Heritage Textual Ontology (HTO)</a>. Each place article is represented as a <code>hto:Description</code>, annotated with quality metrics, provenance (e.g., edition, volume, page), and extraction method (GPT-4). This step is implemented using Python mapping scripts and SPARQL templates.</p>

        <h3 id="semantic-enrichment-and-linking">Semantic Enrichment and Linking</h3>

        <p>Following RDF generation, the knowledge base is semantically enriched through:</p>

        <ul>
          <li><strong>Concept Clustering</strong>: Sentence embeddings and clustering algorithms identify semantically related articles across editions. Resulting clusters form <code>hto:Concept</code> instances representing diachronic place descriptions.</li>
          <li><strong>Entity Linking</strong>: Articles are linked to <a href="https://www.wikidata.org">Wikidata</a> and <a href="https://www.dbpedia.org">DBpedia</a> using hybrid embedding and string-matching approaches.</li>
          <li><strong>Geospatial Annotation</strong>: Named entity recognition (NER) and the Edinburgh Geoparser are used to resolve and annotate place names with coordinates and place types, represented as <code>hto:GeographicAnnotation</code>.</li>
        </ul>

        <h3 id="knowledge-graph-serialization-and-deployment">Knowledge Graph Serialization and Deployment</h3>

        <p>Final outputs are serialized in RDF/Turtle and deployed via a public <a href="http://query.frances-ai.com/hto_gazetteers">Fuseki SPARQL endpoint</a>. All steps are implemented in executable Python scripts and Jupyter notebooks. Each stage is also represented in the RDF using HTO’s provenance properties (e.g., <code>prov:wasGeneratedBy</code>, <code>hto:hasTextQuality</code>), ensuring transparency and reproducibility.</p>

        <p>A pipeline overview and walkthrough are available in the <a href="https://github.com/francesNLP/MappingChange">MappingChange GitHub repository</a>, along with instructions for re-running each step. When gazetteers span multiple volumes (e.g., 1838, 1842), the outputs are merged using dedicated scripts into a single edition-level DataFrame prior to RDF conversion.</p>

      </div>
</section>

  <section id="usage" inlist="" rel="schema:hasPart" resource="#usage">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Usage</h2>

        <p>Mapping Change can be explored in three main ways:</p>

        <h3 id="data-access">Data Access</h3>
        <ul>
          <li>All cleaned DataFrames and RDF graphs are in the GitHub repository: <a href="https://github.com/francesNLP/MappingChange">francesNLP/MappingChange</a></li>
          <li>Scripts for reproducing those dataframes, KGs and ES are in the GitHub repository: <a href="https://github.com/francesNLP/MappingChange">francesNLP/MappingChange</a></li>
          <li><a href="https://zenodo.org">Zenodo DOI</a> (to be added)</li>
        </ul>

        <h3 id="sparql-querying">SPARQL Querying</h3>
        <ul>
          <li>A Fuseki SPARQL server supports knowledge graph exploration.</li>
          <li>Sample queries for retrieving places, concepts, and links are included.</li>
        </ul>

        <h3 id="frances-platform">Frances Platform</h3>
        <ul>
          <li>Users can search and explore articles via full-text or semantic search.</li>
          <li>Concepts are visualized through timelines and embeddings.</li>
        </ul>

        <h3 id="notebooks">Notebooks</h3>
        <p>Google Colab notebooks are provided for each gazetteer to enable direct exploration and analysis.</p>

      </div>
</section>

  <section id="conclussion" inlist="" rel="schema:hasPart" resource="#conclussion">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Conclusion</h2>

        <p><em>Mapping Change</em> creates a temporal, semantic infrastructure for exploring Scottish place descriptions from 1803–1901. Combining LLM-based extraction, improved ontology design, and semantic search, we deliver a reusable, interoperable dataset for historical research.</p>

        <p>The improved HTO ontology enables robust modeling of textual provenance, record quality, and evolving concepts. The Frances platform empowers researchers to query and visualize this data across time and space.</p>

        <p>Future work includes integrating cartographic metadata, and link it to the 100 years of the Encyclopaedia Britannica.</p>

      </div>
</section>

  <section id="acknowledgements" inlist="" rel="schema:hasPart" resource="#acknowledgements">
<div datatype="rdf:HTML" property="schema:description">
        <h2 property="schema:name">Acknowledgements</h2>

        <p>This work was supported by the Royal Society of Edinburgh (RSE Small Research Grant).</p>

      </div>
</section>


</main>


<footer><section>
<h2 id="references">References</h2>
<dl class="references">
  <dt id="ref-1">[1]</dt>
  <dd resource="https://dx.doi.org/10.3233/SW-223034" typeof="schema:Article">Hyvönen, E.: Digital humanities on the Semantic Web: Sampo model and portal series. Semantic Web. 14, 729–744 (2023). doi:10.3233/SW-223034</dd>
  <dt id="ref-2">[2]</dt>
  <dd resource="https://dx.doi.org/10.1177/03400352231202506" typeof="schema:Article">Silva, A.L., Terra, A.L.: Cultural heritage on the Semantic Web: The Europeana Data Model. IFLA Journal. 0, 03400352231202506 (0AD). doi:10.1177/03400352231202506</dd>
  <dt id="ref-3">[3]</dt>
  <dd resource="https://dx.doi.org/10.1109/eScience51609.2021.00012" typeof="schema:Article">Filgueira, R., Grover, C., others: Extending defoe for the efficient analysis of historical texts at scale. In: 2021 IEEE 17th International Conference on eScience (eScience). pp. 21–29 (2021). doi:10.1109/eScience51609.2021.00012</dd>
  <dt id="ref-4">[4]</dt>
  <dd resource="https://dx.doi.org/10.48550/arXiv.2303.08774" typeof="schema:CreativeWork">OpenAI: GPT-4 Technical Report. <a href="https://arxiv.org/abs/2303.08774">https:/​/​arxiv.org/abs/2303.08774</a> (2023). doi:10.48550/arXiv.2303.08774</dd>
  <dt id="ref-5">[5]</dt>
  <dd resource="#ebontology" typeof="schema:CreativeWork">Filgueira, R.: Encyclopaedia Britannica Ontology. <a href="https://w3id.org/eb">https:/​/​w3id.org/eb</a> (2023).</dd>
  <dt id="ref-6">[6]</dt>
  <dd resource="#nlsontology" typeof="schema:CreativeWork">Filgueira, R.: National Library of Scotland Ontology. <a href="https://w3id.org/nls">https:/​/​w3id.org/nls</a> (2023).</dd>
  <dt id="ref-7">[7]</dt>
  <dd resource="#hto" typeof="schema:CreativeWork">Yu, L., Filgueira, R.: The Heritage Textual Ontology (HTO). <a href="https://github.com/frances-ai/HeritageTextOntology">https:/​/​github.com/frances-ai/HeritageTextOntology</a> (2024).</dd>
  <dt id="ref-8">[8]</dt>
  <dd resource="https://www.w3.org/TR/prov-o/" typeof="schema:CreativeWork">Groth, P., others: PROV-Overview. W3C, <a href="https://www.w3.org/TR/prov-o/">https:/​/​www.w3.org/TR/prov-o/</a> (2013).</dd>
  <dt id="ref-9">[9]</dt>
  <dd resource="https://dx.doi.org/10.1109/E-SCIENCE62913.2024.10678663" typeof="schema:Article">Yu, L., Charlton, A., Terras, M., Filgueira, R.: Advancing frances: New Heritage Textual Ontology, Enhanced Knowledge Graphs, and Refined Search Capabilities. In: 20th IEEE International Conference on e-Science, e-Science 2024, Osaka, Japan, September 16-20, 2024. pp. 1–10. IEEE (2024). doi:10.1109/E-SCIENCE62913.2024.10678663</dd>
  <dt id="ref-10">[10]</dt>
  <dd resource="https://dx.doi.org/10.5281/zenodo.14051678" typeof="schema:CreativeWork">Yu, L., Filgueira, R.: Gazetteer_HTO: A Knowledge Graph for representing the Gazetteers of Scotland (1803-1901) following Heritage Textual Ontology . <a href="https://doi.org/10.5281/zenodo.14051678">https:/​/​doi.org/10.5281/zenodo.14051678</a> (2024). doi:10.5281/zenodo.14051678</dd>
</dl>
</section>
</footer>
</div>



</body>
</html>
